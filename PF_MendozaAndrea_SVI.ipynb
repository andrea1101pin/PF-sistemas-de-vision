{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef650d3-f1c5-4bf6-b35d-2f845412a0df",
   "metadata": {},
   "source": [
    "Proyecto final. Sistemas de visión de industrial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f2936-53e9-4aa9-8d84-080db48b1e72",
   "metadata": {},
   "source": [
    "El objetivo principal de este proyecto es desarrollar una aplicación que utilice una cámara web\r\n",
    "para capturar imágenes y procesarlas para detectar un gesto de la mano. En función del gesto,\r\n",
    "el sistema enviará una señal a un microcontrolador Arduino, el cual controlará el encendido de\r\n",
    "un arreglo de LEDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feef02d-1fe7-44c1-880a-5b5e9c1a8f2e",
   "metadata": {},
   "source": [
    "Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5e496-fbb9-46c6-8a72-b51357fcd8ba",
   "metadata": {},
   "source": [
    "En el ámbito de los sistemas de visión industrial, la integración de técnicas de procesamiento de imágenes con el control de hardware ofrece soluciones innovadoras y prácticas para diversas aplicaciones. Este proyecto tiene como objetivo desarrollar un sistema que utilice una cámara web para capturar imágenes en tiempo real, procesarlas y detectar gestos de la mano del usuario. Basado en los gestos reconocidos, el sistema interactúa con un microcontrolador Arduino para controlar el encendido de un arreglo de LEDs.\n",
    "\r\n",
    "La principal motivación de este proyecto es demostrar cómo la visión por computadora puede combinarse con sistemas embebidos para crear interfaces naturales y eficientes. La implementación utiliza Python y OpenCV para el procesamiento de imágenes, mientras que la comunicación con Arduino se realiza mediante la biblioteca pySerial. Este enfoque permite una interacción intuitiva entre el usuario y el sistema, donde gestos simples como una mano cerrada, dedos levantados o movimientos se traducen en señales para controlar los LEDs. El proyecto busca no solo implementar esta funcionalidad, sino también explorar los desafíos asociados con la detección de gestos, las condiciones de iluminación y la optimización de la comunicación hardware-software.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b21569-ea61-4b4b-bc8f-987dd8f89cf9",
   "metadata": {},
   "source": [
    "Descripción del proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d18e9-a428-4276-8563-7d3f53d7ffb9",
   "metadata": {},
   "source": [
    "1.\tCaptura de Imágenes: Utilizarás una webcam para adquirir imágenes en tiempo real de la mano del usuario. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0d7df-1330-4094-8f50-ae468520b5eb",
   "metadata": {},
   "source": [
    "2.\tProcesamiento de Imágenes: Usarás técnicas de procesamiento de imágenes para detectar el gesto de la mano mediante OpenCV y Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d0901-c176-4617-a195-42ab80613d3f",
   "metadata": {},
   "source": [
    "3.\tDetección de Gesto: El sistema debe ser capaz de identificar el gesto de la mano y, dependiendo de este, realizar la siguiente acción: o Si se detecta un gesto de una mano cerrada, o puño, no se encenderá ningún LED. o Si se detecta el gesto del dedo índice levantado, se encenderán un LED. o Si se detecta el gesto del dedo índice y el dedo medio levantados, se encenderán dos LED. o Si se detecta el gesto del de la mano moviéndose, se encenderán todos los LEDs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6050d-f9be-43e1-ab54-fd2fcef7bf0a",
   "metadata": {},
   "source": [
    "4.\tInteracción con Arduino: Utilizarás un Arduino (cualquier modelo compatible, como Arduino Uno) para controlar los LEDs. El Arduino recibirá las señales desde la computadora (donde se está procesando la imagen) a través de una conexión serie (puerto COM). El código de Arduino debe estar configurado para encender los LEDs según la señal recibida. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc53a7-2afd-46a7-a6dd-40df2b4f5ed3",
   "metadata": {},
   "source": [
    "5.\tInterfaz: La comunicación entre el software que procesa las imágenes y Arduino será a través del puerto serie (usualmente con la librería Serial en Arduino y funciones de comunicación en Python, como pySerial o similares)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e8d240-ad3a-4228-b6c3-a5d1552abb4f",
   "metadata": {},
   "source": [
    "1.0.1 1. Importar bibliotecas y crear variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6890cd3-fb25-4911-8745-21fa1288cd0b",
   "metadata": {},
   "source": [
    "En esta parte del código además de crear variables también se importó la biblioteca del Arduino y también se captó desde el puerto COM5 el Arduino, de esta forma se conectó Python con el Arduino para hacer el encendido de los LEDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "id": "f5a48b9d-6487-485e-bd50-9a1fe1a1d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import serial\n",
    "import random\n",
    "arduino = serial.Serial('COM5', 9600, timeout=2)  # Timeout más largo\n",
    "\n",
    "# Mantener el background frame para quitarlo posteriormente\n",
    "background = None\n",
    "# Guarda los datos de la mano para que todos sus detalles estén en un solo␣lugar.\n",
    "hand = None\n",
    "# Variables para contar cuántos fotogramas han pasado y para establecer el␣tamaño de la ventana.\n",
    "frames_elapsed = 0\n",
    "FRAME_HEIGHT = 300\n",
    "FRAME_WIDTH = 400\n",
    "# Prueba a editarlas si tu programa tiene problemas para reconocer tu tono de␣piel.\n",
    "CALIBRATION_TIME = 30\n",
    "BG_WEIGHT = 0.5\n",
    "OBJ_THRESHOLD = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5e527-911c-4130-b2c7-d9aa411e6a8d",
   "metadata": {},
   "source": [
    "1.0.2 2. Clase HandData: Una clase para guardar todos los detalles de la mano y las\r\n",
    "banderas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "id": "e5c3b0cf-6761-487d-b5d3-5bd1f5c51337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandData:\n",
    "    # Atributos de la clase\n",
    "    top = (0, 0)\n",
    "    bottom = (0, 0)\n",
    "    left = (0, 0)\n",
    "    right = (0, 0)\n",
    "    centerX = 0\n",
    "    prevCenterX = 0\n",
    "    isInFrame = False\n",
    "    isWaving = False\n",
    "    fingers = None\n",
    "    gestureList = []\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, top, bottom, left, right, centerX):\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.centerX = centerX\n",
    "        self.prevCenterX = 0\n",
    "        self.isInFrame = False\n",
    "        self.isWaving = False\n",
    "\n",
    "    # Método para actualizar los valores\n",
    "    def update(self, top, bottom, left, right):\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    # Método para verificar si hay un movimiento de \"saludo\"\n",
    "    def check_for_waving(self, centerX):\n",
    "        self.prevCenterX = self.centerX\n",
    "        self.centerX = centerX\n",
    "        if abs(self.centerX - self.prevCenterX) > 3:\n",
    "            self.isWaving = True\n",
    "        else:\n",
    "            self.isWaving = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b9e17d-93a4-4d85-bfac-8c5508c3b10a",
   "metadata": {},
   "source": [
    "1.0.3 3. write_on_image(): Escribir información relacionada con el gesto de la mano\r\n",
    "y delimitar la región de interés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65dde55-497f-4359-b828-86f829d7d4de",
   "metadata": {},
   "source": [
    "En esta sección del código se agregaron 2 puntos. El primero que se agregó un tercer led y el segundo punto se agrega una función “random” para que los LEDs se enciendan aleatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "id": "9ba8d49a-fa98-4309-862f-421c3825faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí tomamos el fotograma actual, el número de fotogramas transcurridos \n",
    "# y cuántos dedos hemos detectado para poder imprimir en la pantalla\n",
    "# qué gesto se está produciendo (o si la cámara se está calibrando).\n",
    "# Inicializar la conexión con el Arduino (ajusta el puerto y la velocidad de baudios según tu configuración)\n",
    "def write_on_image(frame):\n",
    "    global arduino, previous_message\n",
    "    text = \"Buscando...\"\n",
    "    message = \"0\"  # Por defecto, ningún LED encendido\n",
    "\n",
    "    if frames_elapsed < CALIBRATION_TIME:\n",
    "        text = \"Calibrando...\"\n",
    "    elif hand is None or not hand.isInFrame:\n",
    "        text = \"Mano no detectada\"\n",
    "    else:\n",
    "        if hand.isWaving:\n",
    "            text = \"Moviendo\"\n",
    "            message = str(random.randint(1, 3))  # Generar aleatoriamente \"1\", \"2\" o \"3\"\n",
    "        elif hand.fingers == 0:\n",
    "            text = \"Cero\"\n",
    "            message = \"0\"\n",
    "        elif hand.fingers == 1:\n",
    "            text = \"Uno\"\n",
    "            message = \"1\"\n",
    "        elif hand.fingers == 2:\n",
    "            text = \"Dos\"\n",
    "            message = \"2\"\n",
    "        elif hand.fingers == 3:\n",
    "            text = \"Tres\"\n",
    "            message = \"3\"\n",
    "\n",
    "    # Enviar el mensaje al Arduino\n",
    "    arduino.write(f\"{message}\\n\".encode())  # Agregar salto de línea para delimitar\n",
    "\n",
    "    # Mostrar el texto en la imagen\n",
    "    cv2.putText(frame, text, (10, 20), cv2.FONT_HERSHEY_COMPLEX, 0.4, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, text, (10, 20), cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Resaltar la región de interés con un recuadro\n",
    "    cv2.rectangle(frame, (region_left, region_top), (region_right, region_bottom), (255, 255, 255), 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a79b88c-eb91-427f-ac44-8f54adb04ee4",
   "metadata": {},
   "source": [
    "0.1.4 4. get_region(): Separa la región de interés y la prepara para la detección de\r\n",
    "bordes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "id": "ddbd7660-5421-41d9-bd76-03bcbc36b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region(frame):\n",
    "    # Separar la región de interés del resto del fotograma\n",
    "    region = frame[region_top:region_bottom, region_left:region_right]\n",
    "    \n",
    "    # Transforma a escala de grises para que podamos detectar los bordes más fácilmente.\n",
    "    region = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Utiliza un desenfoque gaussiano para evitar que el ruido del fotograma se etiquete como borde.\n",
    "    region = cv2.GaussianBlur(region, (5, 5), 0)\n",
    "    \n",
    "    return region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab353c8-1bff-4d74-9a52-baad61eb7b9c",
   "metadata": {},
   "source": [
    "0.1.5 5. get_average(): Crear una media ponderada del fondo para la diferenciación\r\n",
    "de imágens\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "id": "c9c7d2b5-5353-4890-975f-5fc19fe74b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average(region):\n",
    "    # Tenemos que utilizar la palabra clave global porque queremos editar la variable global.\n",
    "    global background\n",
    "    \n",
    "    # Si aún no hemos capturado el fondo, haz que la región actual sea el fondo.\n",
    "    if background is None:\n",
    "        background = region.copy().astype(\"float\")\n",
    "        return\n",
    "    \n",
    "    # De lo contrario, añade este fotograma capturado a la media de los fondos.\n",
    "    cv2.accumulateWeighted(region, background, BG_WEIGHT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a88c4-c1c1-442d-9e0e-714c7bea713d",
   "metadata": {},
   "source": [
    "0.1.6 6. segment(): Utilizar la diferenciación de imágenes para separar la mano del\r\n",
    "fond\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "id": "3107f79f-58fd-4a97-a6dc-6b42d7b3fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí utilizamos la diferenciación para separar el fondo del objeto de interés.\n",
    "def segment(region):\n",
    "    global hand\n",
    "    \n",
    "    # Encuentra la diferencia absoluta entre el fondo y el fotograma actual.\n",
    "    diff = cv2.absdiff(background.astype(np.uint8), region)\n",
    "    \n",
    "    # Umbral de esa región con un estricto 0 o 1, por lo que sólo el primer plano se mantiene.\n",
    "    thresholded_region = cv2.threshold(diff, OBJ_THRESHOLD, 255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "    # Obtener los contornos de la región, que devolverá un contorno de la mano.\n",
    "    (contours, _) = cv2.findContours(thresholded_region.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Si no conseguimos nada, no hay mano.\n",
    "    if len(contours) == 0:\n",
    "        if hand is not None:\n",
    "            hand.isInFrame = False\n",
    "        return None  # Devuelve explícitamente None si no se encuentra una mano.\n",
    "    \n",
    "    # En caso contrario, devuelve una tupla de la mano rellenada (thresholded_region),\n",
    "    # junto con el contorno (segmented_region).\n",
    "    else:\n",
    "        if hand is not None:\n",
    "            hand.isInFrame = True\n",
    "        segmented_region = max(contours, key=cv2.contourArea)\n",
    "        return (thresholded_region, segmented_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c59d1-d902-46e9-8bd2-4ce424bbf4cb",
   "metadata": {},
   "source": [
    "0.1.7 7. get_hand_data(): Busca las extremidades de la mano y colócalas en el\r\n",
    "objeto global man\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "id": "c08f5f06-8ff0-4a8e-b62d-af24194047a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hand_data(thresholded_image, segmented_image):\n",
    "    global hand\n",
    "    # Encierra el área alrededor de las extremidades en un \"convex hull\" para␣conectar todos los afloramientos.\n",
    "    convexHull = cv2.convexHull(segmented_image)\n",
    "    # Encuentra las extremidades del \"convex hull\" y guárdalas como puntos.\n",
    "    top = tuple(convexHull[convexHull[:, :, 1].argmin()][0])\n",
    "    bottom = tuple(convexHull[convexHull[:, :, 1].argmax()][0])\n",
    "    left = tuple(convexHull[convexHull[:, :, 0].argmin()][0])\n",
    "    right = tuple(convexHull[convexHull[:, :, 0].argmax()][0])\n",
    "    # Obtiene el centro de la palma, así podremos comprobar si ondea y␣encontrar los dedos.\n",
    "    centerX = int((left[0] + right[0]) / 2)\n",
    "    # Ponemos toda la información en un objeto para extraerla facilmente\n",
    "    if hand == None:\n",
    "        hand = HandData(top, bottom, left, right, centerX)\n",
    "    else:\n",
    "        hand.update(top, bottom, left, right)\n",
    "    # Sólo comprueba la ondulación cada 6 fotogramas.\n",
    "    if frames_elapsed % 6 == 0:\n",
    "        hand.check_for_waving(centerX)\n",
    "    # Contamos el número de dedos cada frame,\n",
    "    # para no equivocarse, espera que pasen 12 frames\n",
    "    hand.gestureList.append(count_fingers(thresholded_image))\n",
    "    if frames_elapsed % 12 == 0:\n",
    "        hand.fingers = most_frequent(hand.gestureList)\n",
    "        hand.gestureList.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b0db3-9531-43c7-84fa-81bca4092f00",
   "metadata": {},
   "source": [
    "0.1.8 8. count_fingers(): Cuenta el número de dedos utilizando una línea que cruce\r\n",
    "las puntas de los dedo\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "id": "f648c4d1-e525-4d13-89c4-722c59fe9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_fingers(thresholded_image):\n",
    "    # Encuentra la altura a la que trazaremos la línea para contar los dedos.\n",
    "    line_height = int(hand.top[1] + (0.2 * (hand.bottom[1] - hand.top[1])))\n",
    "    \n",
    "    # Obtiene la región lineal de interés a lo largo de donde estarían los␣dedos.\n",
    "    line = np.zeros(thresholded_image.shape[:2], dtype=int)\n",
    "    \n",
    "    # Traza una línea a través de esta región de interés, donde deberían estar␣los dedos.\n",
    "    cv2.line(line, (thresholded_image.shape[1], line_height), (0, line_height),255, 1)\n",
    "    \n",
    "    # Hace una operación AND a nivel de bit para encontrar el punto en el que la línea cruza la mano: aquí es donde están los dedos.\n",
    "    line = cv2.bitwise_and(thresholded_image, thresholded_image, mask = line.astype(np.uint8))\n",
    "    \n",
    "    # Obtiene los nuevos contornos de la línea. Los contornos son básicamente pequeñas líneas formadas por huecos\n",
    "    # en la línea grande a través de los dedos, por lo que cada uno sería un␣dedo a menos que sea muy ancho.\n",
    "    (contours, _) = cv2.findContours(line.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    fingers = 0\n",
    "    \n",
    "    # Cuente los dedos asegurándose de que las líneas de contorno son del «tamaño de un dedo», es decir, no demasiado anchas.\n",
    "    # Así se evita que un gesto de «mano cerrada» se confunda con un dedo.\n",
    "    for curr in contours:\n",
    "        width = len(curr)\n",
    "        if width < 3 * abs(hand.right[0] - hand.left[0]) / 4 and width > 5:\n",
    "            fingers += 1\n",
    "    return fingers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389c0d0-7c32-42eb-85f9-3486c9174e06",
   "metadata": {},
   "source": [
    "0.1.9 9. most_frequent(); Regresa el valor que aparece más frecuentemente en una\r\n",
    "list\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "id": "5e5914e0-0a57-4427-a0af-60c443ed108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(input_list):\n",
    "    dict = {}\n",
    "    count = 0\n",
    "    most_freq = 0\n",
    "    \n",
    "    for item in reversed(input_list):\n",
    "        dict[item] = dict.get(item, 0) + 1\n",
    "        if dict[item] >= count :\n",
    "            count, most_freq = dict[item], item\n",
    "    return most_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70218948-e794-4d19-851d-84500746e337",
   "metadata": {},
   "source": [
    "0.1.10 10. Main function: Obtener datos de la cámara y llamar a funciones para\r\n",
    "comprenderlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "id": "643c51c1-fbcb-4892-b84c-ba66942409a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuestra región de interés será la parte superior derecha del frame.\n",
    "region_top = 0\n",
    "region_bottom = int(2 * FRAME_HEIGHT / 3)\n",
    "region_left = int(FRAME_WIDTH / 2)\n",
    "region_right = FRAME_WIDTH\n",
    "frames_elapsed = 0\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Guarda el fotograma de la captura de vídeo y redimensiónalo al tamaño de la ventana.\n",
    "    ret, frame = capture.read()\n",
    "    frame = cv2.resize(frame, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "    \n",
    "    # Voltea el marco sobre el eje vertical para que funcione como un espejo, lo que resulta más intuitivo para el usuario.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Separa la región de interés y la prepara para la detección de bordes.\n",
    "    region = get_region(frame)\n",
    "    if frames_elapsed < CALIBRATION_TIME:\n",
    "        get_average(region)\n",
    "    else:\n",
    "        region_pair = segment(region)\n",
    "        if region_pair is not None:\n",
    "            # Si tenemos las regiones segmentadas con éxito, mostrarlas en otra ventana para el usuario.\n",
    "            (thresholded_region, segmented_region) = region_pair\n",
    "            cv2.drawContours(region, [segmented_region], -1, (255, 255, 255))\n",
    "            cv2.imshow(\"Segmented Image\", region)\n",
    "            get_hand_data(thresholded_region, segmented_region)\n",
    "            \n",
    "    # Escribe en la pantalla la acción que realiza la mano y dibuja la región de interés.\n",
    "    write_on_image(frame)\n",
    "\n",
    "    # Muestra el fotograma capturado anteriormente.\n",
    "    cv2.imshow(\"Camera Input\", frame)\n",
    "    frames_elapsed += 1\n",
    "\n",
    "    # Comprueba si el usuario desea salir.\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('x'):  # Salir con 'x'\n",
    "        break\n",
    "    elif key == ord('c'):  # Cerrar cámara con 'c'\n",
    "        print(\"Cerrando la cámara...\")\n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        exit()  # Salimos inmediatamente del programa\n",
    "\n",
    "# Cuando salimos del bucle, también detenemos la captura.\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Cerrar conexión serial\n",
    "arduino.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc6b5f-9df6-4d28-ac0a-c38b3e495631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abb3b38-c5b3-4f0d-ac70-00e4506454da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2114b1-2e76-4dea-935a-5d0427d8fd42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
